# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Philosophy

This is a **personal hobby development project** where **simplicity, maintainability, and readability are the highest priorities**. This is NOT a production system - focus on learning and experimentation.

### Core Principles: YAGNI + KISS

**YAGNI (You Aren't Gonna Need It)**
- Don't add features until you actually need them
- Avoid over-engineering or "future-proofing" 
- Build the simplest thing that works first
- Add complexity only when requirements demand it

**KISS (Keep It Simple, Stupid)**
- Prefer simple, obvious solutions over clever ones
- Use basic, well-understood patterns and libraries
- Avoid unnecessary abstractions or frameworks
- Choose readability over performance unless performance is actually a problem

### What NOT to implement (unless specifically needed)
- Sophisticated error handling or retry logic
- Complex logging systems or monitoring
- Advanced security measures or input validation
- Performance optimizations or caching
- Enterprise-grade database schemas
- Microservices architecture or event systems

### Code Quality Standards
- **Clear responsibility separation**: Each module should have one clear purpose
- **Readable code**: Self-documenting variable/function names
- **Minimal dependencies**: Use standard library when possible  
- **Simple data flow**: Input → Process → Output, avoid complex state management
- **No dead code**: Remove unused files, functions, or features immediately

**コードの説明やコメントは日本語で記述してください。**

## Development Commands

### Main Data Processing Pipeline
```bash
cd src

# Step 1: Prepare base data (scraping, API calls, merging, route calculation)
python make_base_data.py

# Step 2: Generate frontend master data (office area specific datasets)
python make_frontend_master.py

# Manual dependency installation (python-dotenv not in requirements.txt)
pip install python-dotenv
```

**Note**: `make_frontend_master.py` must be run after `make_base_data.py` as it depends on the calculated routes data.

### Web Application
```bash
# Run the web visualization app with Docker
cd mapapp
docker-compose up

# Access the web app at http://localhost:5000
```

## Architecture Overview

This is a dual-component system for analyzing Tokyo area station data:

### 1. Data Pipeline (`src/`)
**Two-stage pipeline:**
1. **Base Data Preparation** (`make_base_data.py`): Collects raw data and performs route calculations
2. **Frontend Data Generation** (`make_frontend_master.py`): Creates office-area-specific datasets for web visualization

**Components:**
- **Scrapers**: `src/scrapers/` - Web scraping for station lists (TravelTowns) and rent prices (SUUMO)
- **API Integrations**: `src/apis/` - Ekispert API for route times, Google Maps API for geocoding
- **Data Processing**: `src/pipeline/` - Merging, filtering, and analysis of collected data
- **Configuration**: `src/config.py` - Environment variables, project constants, and office area definitions

### 2. Web Visualization (`mapapp/`)
- **Backend**: Flask server (`server.py`) with PostgreSQL database
- **Frontend**: Leaflet.js-based map interface
- **Database**: PostgreSQL with station data (coordinates, prices, commute times)
- **Deployment**: Docker Compose setup with auto-initialization

## Data Flow

### Stage 1: Base Data Preparation (`make_base_data.py`)
1. **Collection**: TravelTowns scraper → `data/station_master/station_address.csv`
2. **Geocoding**: Google Maps API → `data/station_master/station_address_with_coordinates.csv`
3. **Enrichment**: SUUMO scraper → `data/price_by_station/price_by_station_{room_type}.csv`
4. **Merging**: Coordinates + Prices → `data/station_coord_price/station_coord_price_{room_type}.csv`
5. **Route Calculation**: Ekispert API → `data/calculated_routes/calculated_routes_{station}.csv`

### Stage 2: Frontend Data Generation (`make_frontend_master.py`)
6. **Office Area Datasets**: Combines routes for specific office areas (Toranomon, Tokyo, Otsuka)
7. **Enrichment**: Adds coordinates, prices, and walking times
8. **Output**: `data/frontend_master/frontend_master_{office}_{room_type}.csv`

### Stage 3: Visualization
9. **Web App**: Loads frontend master data into PostgreSQL for interactive filtering

## Key Configuration

### Environment Variables Required
```
EKISPERT_KEY=your_ekispert_api_key
GOOGLE_MAPS_KEY=your_google_maps_api_key
```

### Critical Missing Constants
The `src/config.py` file is missing these constants that `src/pipeline/analysis.py` expects:
- `MAX_TRANS_DEFAULT`
- `MAX_TIME_DEFAULT` 
- `MAX_PRICE_DEFAULT`

### Target Stations
Currently configured to analyze commute times to stations defined in `WALK_MINUTES` dict in `src/config.py`.

## Data Structure

### CSV Output Files

**Base Data** (generated by `make_base_data.py`):
- `data/station_master/station_address.csv`: Station master (line, station)
- `data/station_master/station_address_with_coordinates.csv`: Station master with geocoding
- `data/price_by_station/price_by_station_{room_type}.csv`: Rent prices by station
- `data/station_coord_price/station_coord_price_{room_type}.csv`: Merged station + coordinates + price
- `data/calculated_routes/calculated_routes_{target_station}.csv`: Route information for each destination

**Frontend Data** (generated by `make_frontend_master.py`):
- `data/frontend_master/frontend_master_toranomon_{room_type}.csv`: Toranomon office area dataset
- `data/frontend_master/frontend_master_tokyo_{room_type}.csv`: Tokyo office area dataset
- `data/frontend_master/frontend_master_otsuka_{room_type}.csv`: Otsuka office area dataset

**Legacy** (no longer generated):
- `data/output/route_info/`: Old route data location
- `data/output/merged/`: Old merged data location

### Database Schema (Web App)
```sql
CREATE TABLE stations (
    id SERIAL PRIMARY KEY,
    line TEXT,
    station TEXT,
    lat DOUBLE PRECISION,
    lng DOUBLE PRECISION,
    price DOUBLE PRECISION,
    commute_time INT
);
```

## Important Notes

### Pipeline Behavior
- **Caching**: Both `make_base_data.py` and `make_frontend_master.py` skip API calls if output files already exist
- **Dependencies**: `make_frontend_master.py` requires `make_base_data.py` to be run first
- **Rate Limiting**: All scraping includes rate limiting (`SCRAPING_SLEEP_SEC`) and retry logic

### File Organization
- **Two-stage approach**: Base data preparation is separated from frontend-specific data generation
- **Office areas**: Frontend data is generated for three office areas: Toranomon (8 stations), Tokyo (7 stations), Otsuka (1 station)
- **Hardcoded areas**: Office area definitions in `config.py` are intentionally hardcoded for this personal project

### Web App
- Web app expects a `stations.csv` file in `mapapp/` directory for database initialization
- Can use either `calculated_routes_*.csv` or `frontend_master_*.csv` as input